# Fake News Detection Using NLP

## ğŸ“Œ Project Overview
Fake news has become a major challenge in the digital age, influencing public opinion and spreading misinformation rapidly.  
This project aims to build a **machine learning-based fake news detection system** that classifies news articles as **Fake** or **Real** using **Natural Language Processing (NLP)** techniques.

The model processes textual news data, extracts meaningful features using **TF-IDF**, and applies a **Support Vector Machine (SVM)** classifier to accurately distinguish between fake and real news articles.

---

## ğŸ¯ Objectives
- To analyze and preprocess raw news text data
- To apply NLP techniques for feature extraction
- To build a classification model for fake news detection
- To evaluate the performance of the model using standard metrics
- To test the model on unseen/custom news input

---

## ğŸ“‚ Dataset Description
The dataset is sourced from **Kaggle** and consists of two CSV files:

- **Fake.csv** â†’ Contains fake news articles  
- **True.csv** â†’ Contains real news articles  

### Dataset Features:
- `title` â€“ Title of the news article  
- `text` â€“ Full news content  
- `subject` â€“ Category of the news  
- `date` â€“ Date of publication  

A new column `label` is created:
- `0` â†’ Fake News  
- `1` â†’ Real News  

The datasets are merged and shuffled to ensure unbiased training.

---

## ğŸ› ï¸ Technologies & Tools Used
- **Programming Language:** Python  
- **Development Environment:** Jupyter Notebook  
- **Libraries:**
  - Pandas
  - NumPy
  - Scikit-learn
  - Matplotlib
  - Seaborn
  - Regex (re)

---

## ğŸ” Methodology

### 1ï¸âƒ£ Data Loading
- Load Fake and Real news datasets
- Assign labels and combine them into a single dataset

### 2ï¸âƒ£ Data Preprocessing
- Convert text to lowercase
- Remove punctuation, numbers, and special characters
- Remove extra whitespaces
- Prepare clean textual data for NLP processing

### 3ï¸âƒ£ Feature Extraction (NLP)
- **TF-IDF (Term Frequencyâ€“Inverse Document Frequency)** is used to convert text into numerical vectors
- Stopwords are removed
- Unigrams and bigrams are included to improve contextual understanding

### 4ï¸âƒ£ Train-Test Split
- Dataset is split into:
  - 80% Training data
  - 20% Testing data

### 5ï¸âƒ£ Model Training
- **Support Vector Machine (SVM)** classifier is used
- SVM is effective for high-dimensional text classification tasks

### 6ï¸âƒ£ Model Evaluation
The model is evaluated using:
- Accuracy Score
- Precision
- Recall
- F1-Score
- Confusion Matrix (visualized using heatmap)

### 7ï¸âƒ£ Custom News Testing
- The trained model is tested with user-defined news text to predict whether it is fake or real

---

## ğŸ“Š Results & Performance
- The SVM model achieves **high accuracy** on the test dataset
- Strong precision and recall values indicate reliable classification
- Confusion matrix confirms balanced performance on both classes

---

## ğŸ“ Project Structure
